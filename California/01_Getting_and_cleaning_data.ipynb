{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data of temperatures in California\n",
    "\n",
    "Author: Hugo Carrillo\n",
    "\n",
    "Creation: July 2024\n",
    "\n",
    "This notebook shows the very beginning steps in heatwave detection from meteorological stations data. \n",
    "1. Detection of missing data: quantification and cleaning.\n",
    "2. Selection of stations. \n",
    "3. We generate cleaned data for heatwave detections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "from calendar import monthrange\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) # Suppress specific RuntimeWarnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "METADATA_PATH_AND_FILENAME = 'data/original/cimis_metadata.csv'\n",
    "TEMP_DATA_PATH = 'data/original/'\n",
    "CLEANED_DATA_PATH = 'data/cleaned/'\n",
    "MY_FUNCTIONS_PATH = '../functions/'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Month_Days(year):\n",
    "    return {\"year\": year, \"month_days\": {m: monthrange(year, m)[1] for m in range(1, 13)}}\n",
    "\n",
    "def Tfilter(data, column_label, nperc, year_window_init: int, year_window_end: int):\n",
    "    start_date = data.index[0]\n",
    "    end_date = data.index[-1]\n",
    "    perc_label = 'perc'\n",
    "    Tadd = 0.0\n",
    "    year_window_init = year_window_init\n",
    "    year_window_end = year_window_end\n",
    "    data_temp = data[column_label]\n",
    "\n",
    "    data_threshold = pd.DataFrame(\n",
    "        [],\n",
    "        columns=[perc_label],\n",
    "        index = data.index\n",
    "    )\n",
    "    for year in range(start_date.year, end_date.year + 1):\n",
    "        month_days = Month_Days(year)\n",
    "        for month in month_days[\"month_days\"]:\n",
    "            for day in range(1, month_days[\"month_days\"][month] + 1):\n",
    "                try:\n",
    "                    current_date = datetime(year, month, day)\n",
    "                except ValueError:\n",
    "                    if current_date == start_date:\n",
    "                        current_date = datetime(year, month, day+1)\n",
    "                    if current_date == end_date:\n",
    "                        current_date = datetime(year, month, day-1)\n",
    "                if  current_date >= start_date and current_date <= end_date:\n",
    "                    f_data_temp = data_temp[\n",
    "                        (year_window_init <= data_temp.index.year)\n",
    "                        & (data_temp.index.year <= year_window_end)\n",
    "                        & (data_temp.index.day == day)\n",
    "                        & (data_temp.index.month == month)\n",
    "                    ]\n",
    "                    try:\n",
    "                        data_threshold.loc[datetime(year, month, day), perc_label] = f_data_temp.quantile(\n",
    "                            nperc*0.01, interpolation=\"midpoint\"\n",
    "                        ).values[0]\n",
    "                    except AttributeError:\n",
    "                        data_threshold.loc[datetime(year, month, day), perc_label] = f_data_temp.quantile(\n",
    "                            nperc*0.01, interpolation=\"midpoint\"\n",
    "                        )\n",
    "                    except ValueError:\n",
    "                        data_threshold.loc[datetime(year, month, day-1), perc_label] = f_data_temp.quantile(\n",
    "                            nperc*0.01, interpolation=\"midpoint\"\n",
    "                        ).values[0] + Tadd\n",
    "\n",
    "        smoothed_series = data_threshold.rolling(window=31, center=True).mean()\n",
    "    return smoothed_series + Tadd\n",
    "    #return data_threshold + Tadd\n",
    "\n",
    "def to_format(data, max_temp_lim = 50, add_filter_year = None, filter_by_hist = False, filter = True, dropnans = True):\n",
    "\n",
    "    data[\"Date\"] = pd.to_datetime(data[\"Date\"],format=\"%Y-%m-%d\")\n",
    "\n",
    "    data = data.rename(columns={\"Date\":\"date\"})\n",
    "    if dropnans:\n",
    "        max_temp = data.set_index(\"date\").dropna(subset=[\"DayAirTmpMax\"])[[\"DayAirTmpMax\"]]\n",
    "        min_temp = data.set_index(\"date\").dropna(subset=[\"DayAirTmpMin\"])[[\"DayAirTmpMin\"]]\n",
    "        mean_temp = data.set_index(\"date\").dropna(subset=[\"DayAirTmpAvg\"])[[\"DayAirTmpAvg\"]]\n",
    "    else:\n",
    "        max_temp = data.set_index(\"date\")[[\"DayAirTmpMax\"]]\n",
    "        min_temp = data.set_index(\"date\")[[\"DayAirTmpMin\"]]\n",
    "        mean_temp = data.set_index(\"date\")[[\"DayAirTmpAvg\"]]\n",
    "\n",
    "    max_temp = max_temp.rename(columns={\"DayAirTmpMax\":\"max_temp\"})\n",
    "    min_temp = min_temp.rename(columns={\"DayAirTmpMin\":\"min_temp\"})\n",
    "    mean_temp = mean_temp.rename(columns={\"DayAirTmpAvg\":\"mean_temp\"})\n",
    "\n",
    "    if filter:\n",
    "\n",
    "        if add_filter_year is None:\n",
    "            max_temp = max_temp.drop(max_temp[np.abs(max_temp[\"max_temp\"])>max_temp_lim].index)\n",
    "        else:\n",
    "        #if add_filter_year is not None:\n",
    "            max_temp.loc[(max_temp.index.year < add_filter_year) & (np.abs(max_temp['max_temp']) > max_temp_lim), 'max_temp'] = np.nan\n",
    "            max_temp = max_temp.dropna(subset=[\"max_temp\"])[[\"max_temp\"]]\n",
    "\n",
    "\n",
    "        if filter_by_hist:#add_filter_year is not None:\n",
    "            perc_max = Tfilter(max_temp, 'max_temp', 99.9, add_filter_year, 2023)\n",
    "            perc_max = perc_max.reindex(max_temp.index)\n",
    "\n",
    "\n",
    "            max_temp.loc[(max_temp.index.year < add_filter_year) & (max_temp['max_temp'] > perc_max['perc'] + 10), 'max_temp'] = np.nan\n",
    "\n",
    "        if dropnans:\n",
    "            max_temp = max_temp.dropna(subset=[\"max_temp\"])[[\"max_temp\"]]\n",
    "            min_temp = min_temp.dropna(subset=[\"min_temp\"])[[\"min_temp\"]]\n",
    "            mean_temp = mean_temp.dropna(subset=[\"mean_temp\"])[[\"mean_temp\"]]\n",
    "\n",
    "    concatenated_df = pd.concat([max_temp, min_temp, mean_temp], axis=1)\n",
    "\n",
    "    return concatenated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StationNbr</th>\n",
       "      <th>Name</th>\n",
       "      <th>City</th>\n",
       "      <th>RegionalOffice</th>\n",
       "      <th>County</th>\n",
       "      <th>ConnectDate</th>\n",
       "      <th>DisconnectDate</th>\n",
       "      <th>IsActive</th>\n",
       "      <th>IsEtoStation</th>\n",
       "      <th>Elevation</th>\n",
       "      <th>GroundCover</th>\n",
       "      <th>HmsLatitude</th>\n",
       "      <th>HmsLongitude</th>\n",
       "      <th>ZipCodes</th>\n",
       "      <th>SitingDesc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Fresno/F.S.U. USDA</td>\n",
       "      <td>Fresno</td>\n",
       "      <td>South Central Region Office</td>\n",
       "      <td>Fresno</td>\n",
       "      <td>6/7/1982</td>\n",
       "      <td>9/25/1988</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>340</td>\n",
       "      <td>Grass</td>\n",
       "      <td>36º48'52N / 36.814444</td>\n",
       "      <td>-119º43'54W / -119.731670</td>\n",
       "      <td>93766, 93762, 93761, 93760, 93759, 93755, 9375...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>FivePoints</td>\n",
       "      <td>Five Points</td>\n",
       "      <td>South Central Region Office</td>\n",
       "      <td>Fresno</td>\n",
       "      <td>6/7/1982</td>\n",
       "      <td>12/31/2050</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>285</td>\n",
       "      <td>Grass</td>\n",
       "      <td>36º20'10N / 36.336222</td>\n",
       "      <td>-120º6'46W / -120.112910</td>\n",
       "      <td>93624</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Beach /Santa Cruz CO</td>\n",
       "      <td>Watsonville</td>\n",
       "      <td>South Central Region Office</td>\n",
       "      <td>Santa Cruz</td>\n",
       "      <td>5/30/1982</td>\n",
       "      <td>8/25/1986</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>10</td>\n",
       "      <td>Grass</td>\n",
       "      <td>36º52'50N / 36.880556</td>\n",
       "      <td>-121º47'36W / -121.793330</td>\n",
       "      <td>95077, 95076, 95075, 95019, 95018</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Webb /Santa Cruz CO</td>\n",
       "      <td>Watsonville</td>\n",
       "      <td>South Central Region Office</td>\n",
       "      <td>Santa Cruz</td>\n",
       "      <td>5/30/1982</td>\n",
       "      <td>4/29/1988</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>230</td>\n",
       "      <td>Grass</td>\n",
       "      <td>36º58'21N / 36.9725</td>\n",
       "      <td>-121º43'34W / -121.726110</td>\n",
       "      <td>95077, 95076, 95075, 95019, 95018</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Shafter</td>\n",
       "      <td>Shafter</td>\n",
       "      <td>South Central Region Office</td>\n",
       "      <td>Kern</td>\n",
       "      <td>6/1/1982</td>\n",
       "      <td>12/31/2050</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>360</td>\n",
       "      <td>Grass</td>\n",
       "      <td>35º31'57N / 35.532556</td>\n",
       "      <td>-119º16'54W / -119.281790</td>\n",
       "      <td>93263, 93280, 93388</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   StationNbr                  Name         City               RegionalOffice  \\\n",
       "0           1    Fresno/F.S.U. USDA       Fresno  South Central Region Office   \n",
       "1           2            FivePoints  Five Points  South Central Region Office   \n",
       "2           3  Beach /Santa Cruz CO  Watsonville  South Central Region Office   \n",
       "3           4   Webb /Santa Cruz CO  Watsonville  South Central Region Office   \n",
       "4           5               Shafter      Shafter  South Central Region Office   \n",
       "\n",
       "       County ConnectDate DisconnectDate  IsActive  IsEtoStation  Elevation  \\\n",
       "0      Fresno    6/7/1982      9/25/1988     False          True        340   \n",
       "1      Fresno    6/7/1982     12/31/2050      True          True        285   \n",
       "2  Santa Cruz   5/30/1982      8/25/1986     False          True         10   \n",
       "3  Santa Cruz   5/30/1982      4/29/1988     False          True        230   \n",
       "4        Kern    6/1/1982     12/31/2050      True         False        360   \n",
       "\n",
       "  GroundCover            HmsLatitude               HmsLongitude  \\\n",
       "0       Grass  36º48'52N / 36.814444  -119º43'54W / -119.731670   \n",
       "1       Grass  36º20'10N / 36.336222   -120º6'46W / -120.112910   \n",
       "2       Grass  36º52'50N / 36.880556  -121º47'36W / -121.793330   \n",
       "3       Grass    36º58'21N / 36.9725  -121º43'34W / -121.726110   \n",
       "4       Grass  35º31'57N / 35.532556  -119º16'54W / -119.281790   \n",
       "\n",
       "                                            ZipCodes  SitingDesc  \n",
       "0  93766, 93762, 93761, 93760, 93759, 93755, 9375...         NaN  \n",
       "1                                              93624         NaN  \n",
       "2                  95077, 95076, 95075, 95019, 95018         NaN  \n",
       "3                  95077, 95076, 95075, 95019, 95018         NaN  \n",
       "4                                93263, 93280, 93388         NaN  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all cimis stations information\n",
    "stations = pd.read_csv(METADATA_PATH_AND_FILENAME)\n",
    "stations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n",
      "5\n",
      "91\n",
      "90\n",
      "70\n",
      "62\n",
      "52\n",
      "47\n",
      "39\n",
      "35\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# Reading raw data of selected stations\n",
    "statlist = [43, 5, 91, 90, 70, 62, 52, 47, 39, 35, 6]\n",
    "\n",
    "stations_data_no_filter = {}\n",
    "stations_data_filter_nans = {}\n",
    "stations_data_filter1 = {}\n",
    "stations_data_filter2 = {}\n",
    "dropnans = False\n",
    "\n",
    "for stat in statlist:\n",
    "    print(stat)\n",
    "    station_data_to_read = pd.read_excel(TEMP_DATA_PATH + 'Stat_' + str(stat) + '.xlsx')\n",
    "    stations_data_no_filter[stat] = to_format(station_data_to_read, max_temp_lim=50, add_filter_year=None, filter_by_hist = False, filter = False, dropnans=dropnans)\n",
    "    stations_data_filter_nans[stat] = to_format(station_data_to_read, max_temp_lim=50, add_filter_year=None, filter_by_hist = False, filter = False, dropnans=dropnans)\n",
    "    stations_data_filter1[stat] = to_format(station_data_to_read, max_temp_lim=50, add_filter_year=None, filter_by_hist = False, filter = True, dropnans=dropnans)\n",
    "    if stat == 70 or stat == 52:\n",
    "        stations_data_filter2[stat] = to_format(station_data_to_read, max_temp_lim=50, add_filter_year=2008, filter_by_hist = True, dropnans=dropnans)\n",
    "    else:\n",
    "        stations_data_filter2[stat] = to_format(station_data_to_read, max_temp_lim=50, add_filter_year=None, filter_by_hist = False, dropnans=dropnans)\n",
    "\n",
    "    #save\n",
    "    stations_data_filter2[stat].to_parquet(CLEANED_DATA_PATH + f'Stat_{stat}.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      total      nans       >50   filter2\n",
      "43  13002.0  0.001615  0.000000  0.000000\n",
      "5   12561.0  0.006076  0.000000  0.000000\n",
      "91  12914.0  0.003076  0.000846  0.000000\n",
      "90  12890.0  0.007383  0.001077  0.000000\n",
      "70  13002.0  0.002538  0.010306  0.000692\n",
      "62  13002.0  0.024919  0.001384  0.000000\n",
      "52  13002.0  0.008537  0.006845  0.002230\n",
      "47  12814.0  0.009229  0.000000  0.000000\n",
      "39  13002.0  0.002769  0.000000  0.000000\n",
      "35  13002.0  0.008460  0.000000  0.000000\n",
      "6   13002.0  0.002538  0.000000  0.000000\n"
     ]
    }
   ],
   "source": [
    "#Percentage of missing and cleaned data\n",
    "\n",
    "df_nans_and_deleted = pd.DataFrame(index=statlist)\n",
    "df_total_days = {}\n",
    "df_filter_nan_days = {}\n",
    "df_filter1_days = {}\n",
    "d_filter2_days = {}\n",
    "total_days = np.zeros((len(statlist),))\n",
    "filter_nan_days = np.zeros((len(statlist),))\n",
    "filter1_days = np.zeros((len(statlist),))\n",
    "filter2_days = np.zeros((len(statlist),))\n",
    "\n",
    "for i, stat in enumerate(statlist):\n",
    "    df_total_days = stations_data_no_filter[stat][stations_data_no_filter[stat].index.year>1988]['max_temp']\n",
    "    #np.isnan(station_data_to_read[stat]['temperature'])\n",
    "    df_filter_nan_days = stations_data_filter_nans[stat][stations_data_filter_nans[stat].index.year>1988]['max_temp']\n",
    "    df_filter1_days = stations_data_filter1[stat][stations_data_filter1[stat].index.year>1988]['max_temp']\n",
    "    df_filter2_days = stations_data_filter2[stat][stations_data_filter2[stat].index.year>1988]['max_temp']\n",
    "\n",
    "    #station_data_to_read[stat][np.isnan(station_data_to_read[stat]['temperature'])]\n",
    "    total_days[i] = len(df_total_days)#[np.isnan(df_total_days)])\n",
    "    filter_nan_days[i] = len(df_filter_nan_days[np.isnan(df_total_days)])\n",
    "    filter1_days[i] = len(df_filter1_days[np.isnan(df_filter1_days)])\n",
    "    filter2_days[i] = len(df_filter2_days[np.isnan(df_filter2_days)])\n",
    "\n",
    "df_nans_and_deleted['total'] = np.array(total_days)\n",
    "df_nans_and_deleted['nans'] = np.array(filter_nan_days)/(np.array(total_days)[0])\n",
    "df_nans_and_deleted['>50'] = -np.array(filter_nan_days)/(np.array(total_days)[0])+np.array(filter1_days)/(np.array(total_days)[0])\n",
    "df_nans_and_deleted['filter2'] = np.array(filter2_days)/(np.array(total_days)[0]) - np.array(filter1_days)/(np.array(total_days)[0])\n",
    "\n",
    "\n",
    "print(df_nans_and_deleted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$\\frac{total missing data}{total days} =$0.007015026718621114\n",
      "$\\frac{total missing data}{total days} =$0.0018693251642777136\n"
     ]
    }
   ],
   "source": [
    "print(r'$\\frac{total missing data}{total days} =$' +\n",
    "      str(np.sum(df_nans_and_deleted['nans']*df_nans_and_deleted['total'])/np.sum(df_nans_and_deleted['total'])))\n",
    "\n",
    "print(r'$\\frac{total missing data}{total days} =$' +\n",
    "      str(np.sum(df_nans_and_deleted['>50']*df_nans_and_deleted['total'])/np.sum(df_nans_and_deleted['total'])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
